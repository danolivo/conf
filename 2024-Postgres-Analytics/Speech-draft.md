# Slide 1: Why did I begin this work?
MIGRATIONS, ANALYTICS, MISSED TECHNOLOGIES.
There are two reasons. The first is that I watch a wave of migrations from Oracle and Microsoft. These systems frequently mature, which means they contain terabytes of data and complicated queries. More importantly, owners can compare their many years of experience directly with enterprises and new Postgres-based configurations.
The second reason is that small and big companies adopt Postgres forks to process analytic queries. Who knows why they do it? Maybe they have enough experience with the database system and don't want to start a new adventure and build competencies around another DBMS without strict necessity. 
Attempting to generalise issues reported by the users, I see that they are primarily about some technologies missed in Postgres. So, some work in that area was necessary to group multiple reports into a limited number of general cases. This is why I am here to present these findings. This wave is still ongoing, and techniques missed in Postgres are arriving, so the research is not finished yet.
For example, right now, I'm stuck in the limits of internal Postgres parameterisation, which sometimes does not allow the use of parameterised joins over subqueries and limits the applicability of the parallel workers' technique.

# Slide 2: Why do I have enough competence to do it properly?
# Slide 3: What was my purpose for doing this?
I have been curious about analytic queries because many courses that are accessible online teach you different things. Sometimes, it is about decision cubes and multidimensional aggregates; sometimes, it is just about obtaining a result or joining multiple relations. So, here is an immediate question for me as a developer: which techniques should I implement to fit analytics better?
The second question was about progress: We have made many commits during the year, but how are they responding to new challenges caused by using Postgres for analytics?
Last but not least, What scope of work must we perform to compete with other systems? Is it possible at all?
# Slide 4:
DATA SEPARATION, FDW TRANSPORT, SOPHISTICATED QUERIES, LIMITED OPTIMISER SUPPORT
When discussing adoption, I meant specific trends and how companies use Postgres. If you analyse YugaByte DB, Crunchy Data, and PostgresPro approaches, you will see that they try to avoid the most challenging problem: storage. They attempt to use as a storage and executor of simple queries some specific database like DocDB or DuckDB and use Postgres as a stateless parser/planner and executor of a whole query. Postgres connects to the next-level database through the foreign data wrapper, pushing down parts of queries simple enough to be executed separately. DBAs and applications benefit from stable and well-known interfaces and protocols. At the same time, they can achieve data scalability by choosing performant storage.
Greenplum, Citus, and Postgres-XL go another way, implementing sharding technology, optimisation, and executor features outside the Postgres core. So, I don't care about these systems.
# Slide 5: So, what is an analytic query?
DATA, AGGREGATE, COMPLEXITY, CTE, INDEXES
I looked after users and what they label analytic queries to understand that. After some time of such surveillance, I found out that people name query 'analytic' if it touches most of the tuples in a table and calculates aggregates (sometimes quite a lot of aggregates) over these data. The query has a complex structure because of multiple joins and because it employs many sortings by different, sometimes intersecting, sets of columns, groupings, and limits.
Also, such queries actively employ subqueries and CTE, making the query hierarchical and with many multi-level references, which adds many problems to optimisation.
The last sign of analytics I found is multiple indexes on a table. These are used to quickly access data in many ways, usually to avoid sort operations. But they can be utilised even during the optimisation stage.
# Slide 6: Current progress.
PROGRESS
Here is the map of key commits made since 2010 that I found and discovered. It is grouped by colours, but of course, this grouping is quite relative.
# Slide 7:
PARTS, fdw, PARALLEL.
If we talk about how Postgres survives the fast growth of storage, here I see the parallel workers technique, which is now used not only for query execution but also in indexing, vacuuming, and extensions. Sometimes, the only parallel workers are an option to compete with the MS SQL Server Query plan. For example, if we need to group billions of rows in a text column, the MS SQL multithreading technique provides a close to linear speedup. It is only a CPU-related operation, and nothing but parallel execution helps. From this standpoint, the invention of parallel append, parallel scan and join is a good development direction.
Having quite rigid legacy storage, the community responded to the challenge with partitioning techniques, making huge progress in partition pruning techniques and speedup planning. Moreover, foreign data wrapping techniques enable the spread of data across servers and parallelise data storage operations.
These techniques work well, and we don't have any complaints or reports on the technological superiority of big enterprises. But the last option—smart caching—is something I have something to talk about.
# Slide 8:
Smart caching gives the optimiser some options to reuse data that has already been read to shared buffers and even to the internal query cache. In PostgreSQL, we have two techniques: Materialization and Memoization.
# Slide 9:
Materialisation is usually used in JOIN operations, likewise shown on the left side of the slide, to cache small relations if predicted that it may be rescanned many times. For example, here we have a join on inequality operator that excludes the possibility of HashJoin. As usual, the best choice here is to scan both relations, sort them, and join them with a MergeJoin operator. But Materialization gives us one more option: if we know that not all the tuples are really needed from the big relation, we can cache smaller relation and just probe each tuple from the outer side in the cache of the inner side. Sometimes, in the case of FULL JOIN, or joining of data of a specific type without a sorting operator, the only way to do it may be NestLoop - in this case, materialisation is highly helpful.
What about competitors? SQL Server implements a sophisticated technique called Spooling that may do the same.
# Slide 10: What more do I want from this feature?
Let's look at the query. Here, you can see the JOIN of two tables on an expression containing subquery. The value of this subquery must be re-calculated on each pair of joining tuples. Query designed to forbid transforming the subquery to join, as PostgreSQL and many other DBMSes can perform.
The sad fact is that the table inside the subquery is a foreign table - we will need to pull tuples through the network each time without at least caching them in shared buffers. You can see such queries even in TPC-H benchmark. Such case slows down query's performance orders of magnitude.
# Slide 11:
Here, the problem is shown. Each FOREIGN scan entry corresponds to a separate backend on the remote side, and each time, they contend reading the same table. I watched cases when execution times rocketed up to almost infinity.
What can we do here? Adding one more subquery transformation technique is palliative because we can't restrict a user (and ORM) with a subset of SQL constructs. However, the solution is not too complex, and it was implemented decades ago in the first versions of Postgres.
# Slide 12:
It is intra-query caching! If a query has foreign tables, it is worth performing an additional pass throughout the query tree, grouping the same table entries, concatenating table filtering conditions, and reading the table only once. In a previous incarnation, the node that implemented this feature was named 'Tee' and can be found in the first Postgres commit in the current git tree.
As you can see in the slide, we will have only one backend on the remote side, reading the table only once. The tuplestore machinery eases the invention of table caching. It also has a significant chance of being implemented as an extension because we have almost all the necessary techniques in the core so far. Is it a core functionality? I am suspicious, but it is not so complicated to be designed and supported as a part of a distributed fork. Also, it is worth an attempt to discuss and implement it in the community.
# Slide 13: Memoize.
It is a pretty impressive technique dedicated to the parameterised subtree problem. It allows the cache of a subtree output if it has parameters or lateral references, and the optimiser predicts 1) this subtree will be rescanned many times and 2) There are not many distinct values of parameters. In that case, it makes no sense to rescan the subtree every time if we know that nothing will be changed. Materialisation doesn't work effectively because the size of the whole result of the subquery is too large.
The example shown on this slide demonstrates the technique. On the left side, you can see a query tree, which is a join of two big tables. Both of them are big enough, but we know we need only one per cent of data from each side. Moreover, we know that one of the tables contains lots of duplicates.
The usual outcome here is the plan with hash join on a table with only two unique values for the joining column. But in that case, we need to pass through the whole other table with a sequential scan. It may be a lengthy operation. Memoize allows us to find a better way: Using Nested Loop, we can scan by index table with only two unique values. Conversely, we can fetch and cache tuples using these values as a parameter. So, because of two unique values in table B, we would need to perform an index scan twice on table A. We will reuse cached tuples obtained from side A with this parameter value for each duplicate extracted from side B. This technique is highly effective when we need to extract only a small portion of data from a big table.
# Slide 14: What more do we want from this technique?
Comparing SQL Server and Postgres query plans, I see room for improvement there. Our implementation is still highly restricted compared to the MS analogue - the Spool node. At first, it can't be used at the top of join nodes for technical reasons. Secondly, it doesn't support the SEMI JOIN case: in that case, JOIN doesn't need any data from one side of the join - it is just the fact that such data exists. MS has a mode of not caching data but the number of tuples. But PostgreSQL implementation must complete the tuple cache from subtree for correct usage. In the case shown on the slide, MS caches the result of an enormous subtree and reuses it during the join, but we rescan this subtree again and again. As I see in user reports, this lack of functionality reduces performance 3-4 times, increasing the number of data pages fetched during the execution.
# Slide 15:
A qualitative difference in query planning is sometimes seen here. Look at the slide. There are three levels of joining. Two of them depend on parameters from Table D. Everywhere NestLoop is used. And it is OK because underlying joins consume and return only one tuple. Even having the possibility to insert a Memoize node over the first or second NestLoop optimiser doesn't have any reason to do that. Postgres plans queries from the bottom up. Planning underlying joins, it knows nothing about rescans induced by the top NestLoop. If you look into the parameters, you can realise that it would make sense: the higher join (x) parameter is unique, and each time, the executor rescans the subquery because of the parameter change. The second parameter (y) has lots of duplicates, and it would be better to cache these parameters and tuples returning.
# Slide 16:
At the same time, the SQL server does different stuff. Look at the picture. It inserts the Spool node over the first join node and caches the resulting tuples. So, 99% of calls will return a cached result instead of a subquery execution. If relations A and B are big enough, it saves time, even when only a few tuples are returned. In this specific case, the SQL server executed four times faster than Postgres, and according to statistics, it employed four times fewer data pages. So, the qualitative superiority here is the knowledge that can be provided only with top-node information. Without the information on Table D's top join type and cardinality, the optimiser never decides to cache the subquery results.
# Slide 17:
So, at first glance, that means, after bottom-up planning, we may need a second step, from the top to the bottom pass. At this pass, we can generate paths knowing upper specifics like the discussed number of rescannings or, as I realised recently, limits on the result's cardinality. I don't know how exactly it is implemented in the SQL server, but we can do it only with such a pass. Fortunately, it can be implemented as an extension, and right now, I have a project dedicated to the implementation of this approach.
# Slide 18:
PURPOSE, REFERENCES
The second core development direction in response to the analytic queries challenge is subqueries and common table expressions.
People usually use them to structure the query and make it more readable. But for the optimiser, this stuff is challenging. Sometimes, because it causes recursive calls, but what is more problematic is references: subquery can reference sources from the upper query and it is a big pain to manage such references and keep consistent, optimising the query with transformations. Also, it all the time questionable how to use them in parallel workers or with highly partitioned tables.
# Slide 19:
All the DBMSes try to do something in this area. Quite useful sometimes query tree transformation rules which transform subquery to a JOIN. Here, the Greenplum Orca optimiser is a famous champion.
Also, there are additional tricks for pre-calculating subquery results (if they are not parameterised) and generating useful statistics on such subqueries because they are planned separately from the query. One more interesting direction is generating different subquery plans and choosing among alternatives while planning the top query.
# Slide 20: EXAMPLE
Here is an example of a correlated query and technique named 'Query unnesting'. If you execute the query 'as is' without transformation, you will have the query plan shown at the top. For each tuple coming from t1, the executor will perform a rescan of t2 because the reference to the value coming from t1 can change the result. To avoid repetitive rescan, a typical optimiser transforms such a plan to SEMI JOIN, as shown below. As you can see, transformation created two clauses. One corresponds to the 'IN' clause, and the second bubbled up from the subquery. It is a highly complicated transformation needing analytical research and proof of correctness, but here, we avoid rescanning with hash join, which builds a hash table over a smaller input and just probes tuples from another input, which is quite a cheap procedure.
# Slide 21: Pro and Cons
WHY IT IS A PROBLEM NOW, ORCA, LIMITS
Although all the DBMSes do such transformations, named 'pull-up', each has only a subset of theoretically described transformations. I think it depends on the architectural limits of specific databases. The champion here is GP Orca, the second (according to my experience) is MS SQL server, and others implement different types of this optimisation. Postgres, in the recent version, introduced a pull-up of simple correlated subqueries (you can see it on the slide), but here, you can find many others that still wait to be implemented. We have ideas and a proposal for a couple of these transformations, but designing others is still a lot of a grind job.
The most challenging part is the transformation's outcome, which may block the optimiser from finding a good execution path. It may also be a more subtle semantic problem.
# Slide 22: What we can do - example?
SHOW THE PROBLEM
For example, let's see this slide. Here, we have a correlated subquery like the above with only one change: instead of the IN operator, we use '='. If you execute such a query in Postgres, you will find out that it is not transformed to join, as shown in the explanation below. It is impossible because of the semantic difference between IN and '=': if the subquery returns a result different from one tuple, an error must be raised. But in the case of the 'IN' operator, we just get false. To do such a transformation, we have to prove somehow that the subquery returns only a single tuple that can be made by a UNIQUE index or aggregate in the target list.
One more question here: What if we don't have an index on the parameterised clause? In that case, we can experience painful degradation.
# Slide 23:
DESCRIBE AND SAY ABOUT SORTINGS
The next part of this section is CTE optimisation. What can we do here? Not so much. But before, limited by computational resources, optimiser planned query and subqueries separately to limit search space. It may be painful. Let's look at an example on the slide. Here is a simple query, which will be transformed into JOIN and CTE; the result will be sorted because of its definition. Having no information about CTE, we treat it as unsorted input for the query.
# Slide 24:
On the slide, you can see the result of such planning: the optimiser chooses NestLoop because one of its inputs is unsorted, and sorting a big table costs a lot. But if it knows about the sort order of the CTE, it can treat both of the join inputs as sorted - just look at the explanation below. And that means that the MergeJoin operator will be more effective here and let us avoid one more sort.
# Slide 25: What's more about subqueries and CTEs?
COMPUTERS POWERFUL, OPTIMAL SUBPLANS
I personally looking forward to breaking the membrane between a Query and its subqueries. I mean, the query should choose among multiple paths of subquery execution, elaborate internal parameterisation, and allow replanting of the subquery if some additional information arrives that wasn't accessible for the subquery.
In user complaints, I see some examples of this flexibility in the MS SQL Server. Dealing with subquery re-optimising and parameterising speeds up queries. And the superiority is not around 2-3 times faster—it is about 2-3 orders of magnitude faster. So, here could be a big breakthrough if the community decides to couple planning.
# Slide 26:
STANDPOINT
The next section is about indexes. People usually think about them as a way to speed up access to data, avoid sorting, and so on. However, as a developer, I look at indexes as a source of statistics, which can help us to improve cardinality estimations. Quite unusual, you think? No one has done it before? And you will be wrong.
# Slide 27:
INEQUALITY EXAMPLE
Indexes have been used to improve estimations since 2010. It happens during the estimation of inequality clauses as shown in a trivial query on the slide in two cases: a histogram is not detailed enough (contains a small number of intervals) and 2) if the value is out of the histogram range or 3).
There are discussions in hackers' mailing lists about the efficacy of this method. The algorithm, according to which the planner makes a probe index scan and touches each tuple in the case of suspicious statistics.
# Slide 28:
I personally think we can use less precise but much more effective methods. For example, with statistics on average column size, we can find the first and last tuple, satisfying conditions. Obtaining the number of blocks covering this data allows us to estimate the number of tuples. Of course, there are visibility issues, but in a predominantly read-only load, it makes sense. Only minor changes in the access method interface are needed to implement this. The selectivity hook allows the implementation of this technique in an extension.
With such a lightweight estimator, we could apply it to equality clauses and, in the case of many indexes, to multi-clause expressions.
# Slide 29:
Aggregates and Window functions. As far as I know, this is the most hated topic in Postgres. The code looks messy compared to other parts of the planner, and only a few people like to discover and change this part of the code.
Unfortunately, analytics is based on aggregates, and this is the most critical part of the query, which influences performance in many ways.
# Slide 30:
So, let's talk about the progress in aggregates. Looking into commits, I found three directions for improvement.
The first employs hash tables. It is almost about hashed grouping algorithms but also about hashed search in arrays and other structures.
The second one is combining aggregates sort orderings. With window functions, we can request aggregate calculation over some subset of data sorted by a specific rule. 
The last one is prosupport machinery - an extension of the planner, allowing users to implement additional logic that can make the optimiser's decisions more precise.
# Slide 31: Hashing.
Right now hashing is used for many purposes, including hashed cache of subplan, hashed storage of Memoize node. But Hashed aggregation, invented relatively recently, influences performance much better.
Hashed aggregation inserts a tuple made from grouped columns into the hash table only once. Next time, when a tuple with the same value of grouped parameters is coming, if it matches one in the hash table, it is just counted and eliminated. So, we can return result before group operation is finished. So, it doesn't break query processing pipe and works especially well for queries with limits. However, the negative consequence of this approach is that it needs additional memory for the hash table. So, if a huge amount of data is predicted, the optimiser frequently chooses sort-based grouping.
# Slide 32:
The next optimisation, introduced recently, was the most difficult to realise because the explain doesn't show any difference. Look at the slide. Here you see two aggregates, calculated by different sort orders and grouping, also implicitly requesting some presorting to group. It doesn't make a lot of sense, just for demonstration.
Each aggregate works separately and accumulates data in some sort of tuple store, sorting it independently. This is the logic of Postgres 13. You can see this behaviour on the left diagram. Here, we spend a lot of additional memory and CPU. The optimisation made by David Rowley discovers all sortings available at the moment of aggregation and attempts to generalise them to reduce the number of storages and sortings.
On the right side of this slide, you can see the current behaviour for this query: aggregates and group-by all need data to be presorted by the 'ten' column. Presorting data by two columns immediately after scanning we do a bit more complicated work but eliminate any additional sort operations at all!
# Slide 33:
To understand the effect, let me show you this slide. Here, you can see the result of the EXPLAIN ANALYZE, made in PG13 and 17. The only difference is in the Sort Key inside the Sort node—the optimiser found that the common sort order is profitable for most aggregates. We have a 10-time speedup, avoiding additional sorting and data storage. Imagine how great the effect could be if this query processed billions of rows and needed disk swap! It is a big leap for analytic queries, but unfortunately, I still have not experimented with this query in other databases to realise which techniques they use to streamline this process.
Unfortunately, aggregate internal operations aren't reflected in the explanation, and they are also a subject that needs to be improved and committed to the core.
# Slide 34: Prosupport machinery.
I think this quiet technique isn't in the limelight. David Rowley designed it in 2019 and is continually improving it. Under the hood lies the fact that functions is the most unknown part for the optimiser. Especially custom-made ones. Sometimes they do a lot of work or return unpredictable number of tuples. And to respond this challenge supporting routines was designed. For each function in pg_proc you can add self-made function and register it as prosupport.
During the planning, the optimiser will check it in the catalogue and, if it finds one, will call it, passing inside all parameters of the function, even the parse tree, implementing the call of this function. So, you can manipulate all these data: return the number of rows, cost or selectivity, predicting it for a specific set of incoming parameters. Or even replace the function call with constant if, with this set of parameters, it will do nothing helpful at all.
# Slide 35: To be more precise let me show you a trivial example.
Here you can see prepared statement with expression calling routine. I have used stupid example getting standard routine from catalogue to simplify stuff. This function just multiply an integer value to a constant. As you can see, default estimator use the same estimation for all the values. But what if we add supporting routine which will simplify the query if parameter is 1 or zero? Let me show the effect on the next slide.
# Slide 36:
Here I designed new C-function, the code you can find in additional materials to this presentation later. As you can see, I have updated catalogue table row, which describe our function. Inside the support function, I analyse the value of the parameter coming to the function. If its value is equal to 0, we can just remove the whole expression because it will be true all the time. If the parameter is 1, we can replace the function definition with x, and estimation drastically improves. In other cases, we return the same expression with no changes.
Of course, we should be careful and be sure that the query plan we are building will not be generalised and cached. In that case, it can be used for different sets of parameters and produce incorrect results.
# Slide 37: How to use
It seems too narrowly usable to explain here. But let me show one trick we sometimes use in complicated cases.
Sometimes we have complicated queries, involving COALESCE expressions or overcomplicated CASE ..WHEN constructions, which are challenging to estimate in all database systems. Sometimes we can wrap such complex subquery inside a function and add some additional logic, usually related to specific of query generation system, to simplify expression or provide correct selectivity estimation.
In cases with CASE.. WHEN constructions it is the only case to provide correct statistics to the optimiser. But looks weird, I admit that.
# Slide 38:
So, what's more we can do here? This technique are unique (at least I didn't found anything similar in Oracle or MS). And here we haven't have any champion and can envision. As I see the progress, we should extend this machinery further. Next obvious step is aggregates. We have a thread about that improvement in the hackers mailing list. One more thing - sometimes we need some query tree transformations - subquery pull-ups, I told you before, COALESCE or BoolExpr transformations - it looks like we might extend this technique to query tree nodes - allowing to register helper function for a node. It may be an interesting trade-off: if a user needs deep query tree transformations, they can define it and do it in place without extra query tree passes. Likewise, extensions do now. However, another user may want to get all the performance from manual query optimisation. And also can obtain that. So, isn't it the best solution?
# Slide 39:
The next topic is query complexity. I separated it because we have some problems related to some stuff which is not obvious for a user but needed to be resolved of we want to get optimal plans.
# Slide 40:
I mean, not only trivial complexity added by bushy query tree, but also cascades of OUTER JOINS mixed with FULL, INNER, and SEMI JOINS. Also, it is necessary to minimise sort operations whenever possible. One example of such sort reduction you have already seen recently, when I told about aggregates. One more big problem is NULLS, not stored on disk but generated by OUTER JOINS. if the optimiser misses this information, it can use NestLoop in the case where tons of tuples with NULL values arrive. And the most debatable subject is complex expressions and multicolumn statistics. All this stuff is discussed in the hackers' mailing list and should be addressed soon.
# Slide 41:
To reduce number of sort operations Tomas Vondra recently invented IncrementalSort. Let me show how it works with a simple example. Imagine, you want to get some data from the table and sort it by three columns. You can find such queries, for example, in TPC-H benchmark. Having no indexes you will just scan the table and make sort explicitly. But if you have additional index - in fact two indexes as shown on this slide - you can choose another strategy - use index and obtain partly sorted data. Before IncrementalSort it doesn't make a lot of sense, because Sort operator will sort tuples selected. But incremental sorting will try to sort groups of tuples with the same value of x by value of y and z. Having smaller groups we save efforts and memory allocated for sorting. Moreover, we can return data as fast as first such group will be sorted out. As you can see, here we have two additional options to sort: use cheaper index but sort on two columns or more expensive index but smaller groups. The decision the optimiser choose depends on the statistics on uniqueness of each column and it is not obvious at the moment.
# Slide 42:
So, let's look at these real examples. Here, you can see that SeqScan is the cheapest way to scan the table. But sorting costs much more, and if you look into the final costs, the optimiser prefers to use expensive indexes but performs fewer sorting operations. Also, you can see how cheap the startup cost is in the last case: it will be useful for cases with the LIMIT operator at the top of the SQL query.
# Slide 43: What's more for the incremental sort?
The sad story is that the optimiser can't differentiate the costs of sorting by the number of columns. Look at the example. As you can see, startup and total costs for both queries are exactly the same. But having more columns in the sort list increases the number of comparison operations. This is a frequent case in my practice, and I can recall many complaints about it.
# Slide 44:
The next hidden problem is NULLs, generated during the query execution. See the schema on the slide. NULLs can bubble up from nothing during OUTER JOIN execution: if a tuple from left side doesn't find a match on the right side it adds NULL at the position of columns from the right side. So, even if table B had only 100 tuples and NOT NULL constraint, in the output, query can provide in the position of B.x 1E5 NULLs. Any upstream operation with this column can cause huge data skews and non-optimal plans. Quite a frequent complaint.
But why not to just remember number of nulls predicted and carry up this knowledge. It is not easy.
# Slide 45:
We have at least two big problems. The first one is levels: we can have multiple OUTER JOINs spreaded throughout the query tree, mixed with inner and semi joins and so on. And we just not have machinery to pass this knowledge through inner joins where nulls generation can't happen at all.
# Slide 46:
The second problem also difficult. Let's see two queries on this slide. It is just a single OUTER JOIN on both sides. But if you look into the explanation, you will see that the first one was executed as LEFT JOIN, but the second one was replaced with INNER JOIN. It is because of slight difference in semantics: in the first query join expression exists in the ON part, second query - in WHERE. That means, second query will filter all nulls arising in OUTER JOIN and we can execute it in INNER semantics. So it is not easy to identify the case of generated NULLs at all.
In v16 Tom Lane added machinery to identify such situations, when NULLs may arise. So, it is quite challenging but interesting and actual project to invent technique which will transfer some sort of statistics on these NULLS through the query plan.
# Slide 47:
The next topic is EXTENDED STATISTICS. It is huge topic and I provided whole lecture in april. But let's talk short, only about analytic queries and current problems.
The big problem of is raising from database denormalisation and following multi-clause filters. Let's see the example. Here you see trivial select by three clauses. The first one has low selectivity, second one - 50% selectivity and the last one 10% selectivity. But how to calculate overall selectivity? Postgres employs uniform distribution conjecture and just multiplies these selectivities.
In most of real cases it is wrong approach tend to underestimations and further NestLoop joins.
# Slide 48:
The only one and quite common solution, that presents in MS, not sure about Oracle and Postgres is extended statistics. It generates three types of statistics over set of columns - MCV, distinct values and dependencies. Calculating these statistics it probe each possible combination of two three and so on columns. So we will have detailed information about inter-column dependencies, doesn't it?
# Slide 49:
But it also creates a new problem. The more columns we add to the set, the more combinations we have. As you can see, the number of combinations is quickly growing. It is a problem for statistic building. Then, I tried to experiment with ANALYZE on this data; I saw how 400 ms of analysis on four columns corresponded to 30 seconds on the same database for eight columns. So, it may be used in a pretty limited way and mostly on read-only databases. MS respond to this challenge with specific background workers, partial analysis and analyse-during-scanning tricks. We don't have so many resources for now.
# Slide 50:
We found an alternative way to use index patterns for the creation of statistics. Let's look into the slide. Here is quite a typical combination of indexes. Here, you can earn at least two quite curious facts:
Really many composite indexes. It looks like these indexes reflect patterns of future queries.
Their column sets are intersected.
The first fact means we have a criteria how to build extended statistics - just discover composite indexes! Moreover, we have the most probable order of clauses coming queries - the index doesn't apply if you don't have a restriction on the first column! Of course, according to this example. We will fall short of the cost of analysing and generating statistics on each index. But the second fact give us a silver lining way - we can analyse indexes and build statistics, covering combinations of these indexes if it is more effective than build statistics for each index independently.
I have already made an extension for automatically building extended statistics to prove this approach. It works and has made an advancement in making extended statistics more compact, but it is still not enough.
# Slide 51:
What's more here? Of course, we need extended statistics for JOIN clauses, and one thread in the hacker's mailing list is almost done with it. Also, we need some in-core options to build custom combinations of extended statistics - as, in the example with indexes, we already have a specific order of clauses. So, we don't need any other combinations of columns - only direct order. It saves a lot of computational resources during a table analysis.
Also, new statistical methods. First, of course, is the multidimensional histogram. The second one—my hope—is the number of distinct values in each interval of the histogram.
If we talk about competitors, MS is a leader here. They have many ways to gather statistics. They have automatic stuff to identify sets of clauses to build statistics and methods to use in scanning filters and joining clauses. Postgres is not too far away. The key to success, in my opinion, is to invent the extensibility of statistics that is good enough to improve it.