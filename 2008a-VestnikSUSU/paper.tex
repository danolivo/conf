\documentclass[12pt,twoside]{article}
\usepackage{inputenc}
\usepackage[english,russian]{babel} %Windows
\usepackage{vestnik}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\setcounter{page}{1}

\begin{document}
\def\Re{\mathop{\rm Re}\,}
\def\Im{\mathop{\rm Im}\,}
\def\dom{\mathop{\rm dom}\,}
\def\dist{\mathop{\rm dist}}
\def\grad{\mathop{\rm grad}}
%
%
\newcommand{\phan}{\hspace*{0cm}}
\newcommand{\comment}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Оформление требуемых сведений
\small{
\begin{flushleft}
\textbf{УДК 004.45}
\end{flushleft}}\vspace{10pt}
\author{А.В. Лепихов}
\title{ИСПОЛЬЗОВАНИЕ МЕТОДА ЧАСТИЧНОГО ЗЕРКАЛИРОВАНИЯ ПРИ БАЛАНСИРОВКЕ ЗАГРУЗКИ В ПАРАЛЛЕЛЬНЫХ СУБД ДЛЯ КЛАСТЕРНЫХ СИСТЕМ}

\maketitle{}
% оформление аннотации
\begin{abstract} \begin{tabular}{p{20mm}p{119mm}}
&\noindent {\footnotesize \qquad 
Cтатья посвящена проблеме балансировки загрузки в параллельных СУБД для кластерных систем. Предложен алгоритм балансировки загрузки основанный на методе частичного зеркалирования. Описана реализация данного алгоритма для операции соединения методом хеширования в оперативной памяти. Описаны результаты вычислительных экспериментов, в которых исследована эффективность предложенного алгоритма балансировки в условиях перекосов в распределении данных.
}
\end{tabular}\end{abstract}

\keywords{Системы баз данных, Параллельная обработка, Балансировка загрузки}

%%%%%%%%%%%%%%%%%%%%
%Верхние колонтитулы
\markboth{А.В. Лепихов}{Балансировка загрузки в параллельных СУБД}

% Заголовки разделов формируются при помощи команд  \section{}, \subsection{}, \subsubsection{}

\section*{Введение}
В настоящее время большое распространение получили относительно недорогие кластерные системы, которые отличаются простотой наращивания вычислительных ресурсов (памяти, дисков, процессоров) и потенциально обладают неограниченной производительностью. В рейтинге TOP500 самых мощных компьютеров мира доля кластерных систем составляет 80\%. В соответствии с этим становится актуальной задача разработки новых параллельных приложений и адаптации существующих приложений к кластерным системам.
В области параллельных систем баз данных одним из наиболее перспективных подходов к созданию параллельных СУБД является метод инкапсуляции параллелизма. Данный подход предполагает создание параллельной СУБД путем внедрения специальных операторов-капсул \texttt{exchange} в последовательный план выполнения запроса. Изначально метод инкапсуляции параллелизма был разработан для многопроцессорных систем с общей памятью~\cite{B_Graefe93}. Затем он был обобщен для мультипроцессоров с массовым параллелизмом и для кластерных систем~\cite{B_Sok01}.
Основной проблемой для кластерных систем является проблема перекосов~\cite{B_Maertens01}. Эффект перекоса состоит в неравномерной загрузке процессорных узлов. Основной причиной возникновения перекосов в системах баз данных является неравномерное распределение данных по процессорным узлам. Перекосы могут приводить к полной деградации общей производительности системы~\cite{B_Lakshmi90}.
В данной работе исследуется метод частичного зеркалирования~\cite{B_KostenetskyLS07}, который может быть эффективно использован для балансировки загрузки в кластерных системах. Предлагается оригинальный алгоритм балансировки загрузки, основанный на использовании метода частичного зеркалирования и оператора \texttt{exchange}. Описывается реализация этого алгоритма для операции соединения хешированием в основной памяти. Данный алгоритм реализован в прототипе параллельной СУБД Омега. Исследована эффективность предложенного алгоритма балансировки в условиях перекосов в распределении данных по процессорным узлам.
\hspace{0.7 cm}
\section{Метод частичного зеркалирования}
В основе метода частичного зеркалирования лежит \textit{функция фрагментации}~\cite{B_Sok01} $\phi_{X}, \phi_{X}:X \rightarrow N$, где $X$ -- произвольное отношение. Для любого $x \in X$ значение $\phi_{X}(x)$ определяет номер узла, на котором хранится кортеж $x$.
Через $X_{k}$ будем обозначать фрагмент отношения $X$, хранящийся на k том узле: $X_{k}=\{ x|x\in X, \phi_X(x)=k\} $. через $X_{k}^{m}$ будем обозначать реплику $X_{k}$ хранящуюся на $m$-том узле. В частности, имеем $X_k^k=X_k$.
Мы предполагаем, что каждое отношение фрагментировано по $K$ узлам. Мы также предполагаем, что каждый фрагмент реплицирован (возможно, частично) на $M$ узлах. Нумерация узлов начинается с единицы. Фрагмент состоит из набора \textit{сегментов} -- блоков кортежей заданного размера. Размер реплики $k$-того фрагмента на $m$-том узле задается коэффициентом репликации $\rho_k^m$. Поскольку отношение $S$ значительно больше отношения $R$ то мы будем предполагать, что балансировка выполняется только для отношения $S$.
Балансировка происходит с точностью до сегмента. При этом в балансировке, в качестве рабочих узлов, могут принимать участие только те узлы, на которых имеются фрагменты исходных отношений.
\begin{figure}[!tb]
\begin{center}
\includegraphics[scale=0.3]{images/P_1.eps}\caption{Балансировка загрузки для двух узлов с $\rho=0.5$.}\label{P_1}
\end{center}
\end{figure}
Опишем механизм балансировки загрузки для произвольной унарной однопроходной операции над отношением $S$. Схема работы предлагаемого алгоритма изображена на рис.~\ref{P_1} на примере кластера с двумя процессорными узлами и коэффициентом репликации 0.5.
Пусть процессоры $P_1$ и $P_2$ выполняют выборку кортежей из отношения $S$. Фрагмент $S_1$ назначается процессору $P_1$, фрагмент $S_2$ -- процессору $P_2$. Пусть в момент времени $t_2$ процессор $P_1$ уже обработал выделенные ему фрагменты, в то время как процессор $P_2$ выполнил только часть назначенной ему работы.
В этом случае происходит перераспределение оставшейся необработанной части фрагмента $S_2$. Причем процессор $P_1$ использует для обработки реплику $S_2^1$. Процесс продолжается до тех пор, пока тестируемое отношение $S$ не будет полностью обработано. Алгоритм очевидным образом обобщается на произвольное количество процессоров. Перераспределение необработанной части фрагмента может быть произведено заново, если к моменту времени $t_4$ процессор $P_2$ не завершит обработку своего фрагмента.

\hspace{0.7 cm}
\section{Соединение хешированием в основной памяти}
Современные кластерные системы имеют значительную суммарную оперативную память. Например, кластер СКИФ Урал~\cite{B_Skif} имеет суммарную оперативную память 1.33 ТБ. В контексте систем баз данных это означает, что при использовании фрагментного параллелизма отношение размером меньше 1.33 ТБ может быть фрагментировано по процессорным узлам таким образом, что каждый фрагмент этого отношения целиком поместится в оперативную память. В бинарных реляционных операциях обычно одно из отношений по размеру значительно превосходит другое. Применительно к операции соединения меньшее отношение называется \textit{опорным}, а большее -- \textit{тестируемым}. В реальных приложениях баз данных случаи, когда опорное отношение имеет размер более терабайта, достаточно редки. Поэтому в кластерных системах для операции соединения может быть использован простой и эффективный алгоритм соединения методом хеширования в основной памяти~\cite{B_DeWitt85}.
Для реализации балансировки загрузки в набор операций реляционной алгебры добавляется специальный оператор \texttt{exchange}~\cite{B_Sok01}. Оператор \texttt{exchange} выполняет перераспределение кортежей между процессорными узлами в соответствии с \textit{функцией распределения}, которая для каждого кортежа определяет номер процессорного узла, на котором данный кортеж должен быть обработан.
\begin{figure}[!tb]
\begin{center}
\includegraphics[scale=0.48]{images/P_2.eps}\caption{Параллельный план запроса $R\rhd\lhd S$.}\label{P_2}
\end{center}
\end{figure}

Пример использования оператора \texttt{exchange} показан на рис.~\ref{P_2}. На данном рисунке представлен параллельный план запроса, реализующего операцию MHJ соединения хешированием в основной памяти. Оператор \texttt{scan} выполняет сканирование отношения. Оператор \texttt{exchange} вставляется в качестве левого и правого сына оператора \texttt{MHJ}. В процессе обработки запроса он выполняет перераспределение кортежей, поступающих от оператора scan, между процессорными узлами.
Общую схему обработки операции соединения хешированием в основной памяти можно описать следующим образом. Параллельный план (см. рис.~\ref{P_2}) передается на каждый процессорный узел, содержащий фрагмент базы данных. Параллельный агент, расположенный на узле, выполняет данный параллельный план.
Процесс выполнения соединения можно разделить на две фазы. На первой фазе выполняется инициализация операции соединения. Кортежи опорного отношения перераспределяются между процессорными узлами. Параллельный агент строит фрагмент хеш-таблицы опорного отношения в основной памяти в соответствии с некоторой хеш-функцией.
На второй фазе выполняется обработка тестируемого отношения. Кортежи тестируемого отношения перераспределяются между узлами, и выполняется операция соединения.
Операция формирования хеш-таблицы не требует балансировки ввиду небольшого размера опорного отношения. Таким образом, при выполнении операции соединения методом хеширования в основной памяти балансировка используется на этапе обработки тестируемого отношения и сводится к балансировке загрузки унарной однопроходной операции, описанной выше.
\hspace{0.7 cm}
\section{Результаты экспериментов}
Метод балансировки загрузки реализован в прототипе параллельной СУБД "<Омега">~\cite{B_Omega}. На базе данного прототипа проведены вычислительные эксперименты с целью оценки эффективности предложенного метода балансировки при выполнении операции соединения и оценка оптимального значения коэффициента репликации~$\rho$. Испытания проводились на вычислительном кластере ЮУрГУ - СКИФ Урал~\cite{B_Skif}. Основные системные параметры кластера приведены в табл.~\ref{T_1}.
\begin{table}[h]
\begin{center}
\caption{Параметры системы баз данных}
\medskip
\begin{tabular}{|l|r|}
\hline
Параметр & Значение\\
\hline
Количество процессорных узлов  & 166\\
\hline
Частота ядра процессора & 3.0 GHz\\
\hline
Оперативная память & 1.33 TБ\\
\hline
Дисковая память & 49.29 TБ\\
\hline
Пропускная способность сети & 20 Гбит/сек\\
\hline
Размер отношения $R$ & 0.4 ГБ\\
\hline
Размер отношения $S$ & 8 ГБ\\
\hline
Размер сегмента & 2 МБ\\
\hline
\end{tabular}
\label{T_1}
\end{center}
\end{table}

В экспериментах использовалась тестовая база данных, состоящая из отношений с целочисленными атрибутами. Для генерации значений атрибутов использовалось распределение "<80--20">, "<45--20"> и нормальное распределение. По умолчанию, все атрибуты генерировались в соответствии с распределением "<80--20"> согласно которому, двадцати процентам значений атрибута соответствует восемьдесят процентов кортежей~\cite{B_Heising63}.
В экспериментах исследовалось влияние механизма балансировки на время выполнения операции соединения хешированием в основной памяти. В ходе эксперимента прототип "<Омега"> запускался на 20 процессорных узлах кластера СКИФ Урал с различным коэффициентом репликации $\rho$. Коэффициент репликации варьировался в пределах от 0 (отсутствие репликации) до 1 (полная копия базы данных на каждом узле). Эксперименты показали, что при использовании механизма балансировки загрузки наблюдается значительное увеличение скорости выполнения операции соединения. Результаты данного эксперимента изображены на рис.~\ref{P_3}.
\begin{figure}[!bt]
\begin{center}
\includegraphics{images/P_3.eps}\caption{Зависимость времени выполнения запроса от коэффициента репликации~$\rho$}\label{P_3}
\end{center}
\end{figure}
Наибольший прирост производительности возникает при выполнении запросов над базой данных, сгенерированной и фрагментированной в соответствии с правилом "<80-20">. Коэффициент репликации 0.15 дает прирост производительности 6\%, При $\rho$=0.60, прирост производительности составляет 25\%, а при полной репликации ($\rho$=1.00) -- 35\%. Таким образом, в конкретных приложениях баз данных коэффициент репликации можно варьировать с целью уменьшения накладных расходов на поддержание реплик.
В настоящее время ведутся работы по реализации метода балансировки для алгоритмов GRACE соединения и гибридного соединения. Планируется внедрение предложенного метода балансировки загрузки в СУБД с открытым исходным кодом PostgreSQL.

{\it Работа выполнена при финансовой поддержке Российского фонда фундаментальных исследований (проект 06-07-89148).}
% оформление библиографии
\begin{biblio}

\bibitem{B_Graefe93} Graefe G. Encapsulation of Parallelism in the Volcano Query Processing Systems / Graefe G. // Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data, Atlantic City, NJ, May 23--25, 1990. ACM Press. --1990. --P. 102--111.\vspace{-2mm}

\bibitem{B_Sok01} Соколинский Л.Б. Организация параллельного выполнения запросов в многопроцессорной машине баз данных с иерархической архитектурой / Соколинский Л.Б. // Программирование. --2001. --No. 6. --С. 13--29.\vspace{-2mm}

\bibitem{B_Maertens01} Maertens H. A Classification of Skew Effects in Parallel Database Systems / Maertens H. // Proceedings of 7th International Euro-Par Conference, Manchester, UK, August 28--31, 2001. --P. 291--300.\vspace{-2mm}

\bibitem{B_Lakshmi90} Lakshmi M.S. Effectiveness of Parallel Joins / Lakshmi M.S., Yu P.S. // IEEE Transactions on Knowledge and Data Engineering. --1990. --Vol. 2, No. 4. --P. 410--424.\vspace{-2mm}

\bibitem{B_KostenetskyLS07} Костенецкий П.С. Технологии параллельных систем баз данных для иерархических многопроцессорных сред / Костенецкий П.С., Лепихов А.В., Соколинский Л.Б. // Автоматика и телемеханика. --2007. --Том 68, No. 5. --С. 847--859.\vspace{-2mm}

\bibitem{B_DeWitt85} DeWitt D.J. Multiprocessor Hash-Based Join Algorithms / DeWitt D.J., Gerber R.H. // VLDB'85, Proceedings of 11th International Conference on Very Large Data Bases, Stockholm, Sweden, August 21--23, 1985. --Morgan Kaufmann. --1985. --P. 151--164.\vspace{-2mm}

\bibitem{B_Omega} Прототип параллельной СУБД "<Омега"> : [http://omega.susu.ru]\vspace{-2mm}

\bibitem{B_Skif} Высокопроизводительный вычислительный кластер "<СКИФ Урал"> : [http://skif-ural.susu.ac.ru]\vspace{-2mm}

\bibitem{B_Heising63} Heising W.P. Note on Random Addressing Techniques // IBM System Journal. --1963. --Vol. 2, No. 2. --P. 112--116.

\end{biblio}

 

\address{Кафедра "<Системное программирование">, \\ ЮУрГУ \\ lepihov@gmail.com}
%%%%%%%%%%%%%%%%%%%%
%оформление английской версии названия, авторства, аннотации, ключевых слов
\author{A.V. Lepikhov}

\title{Replication-based Load Balancing Algorithm for Main Memory Hash-joins in Shared-Nothing Systems}
% Для построения заголовка статьи используйте команду \maketitle{}
\maketitle{}
\begin{abstract} \begin{tabular}{p{20mm}p{119mm}}
&\noindent {\footnotesize \qquad This Paper is dedicated to load balancing problem in parallel database management systems for cluster-based architectures. Load balancing algorithm based on partial mirroring method is described. Implementation of load balancing algorithm is presented. Experimantal results demonstrates effectivness of the proposed algorithm.
языке} 
\end{tabular}\end{abstract}

\keywordsanglish{load balancing, hash-joins, parallel processing}


\end{document}
